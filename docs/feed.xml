<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Yet Another Lambda Blog</title>
        <link>https://jstolarek.github.io</link>
        <description><![CDATA[A language that doesn't affect the way you think about programming, is not worth knowing]]></description>
        <atom:link href="https://jstolarek.github.io/feed.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Mon, 23 Jan 2023 00:00:00 UT</lastBuildDate>
        <item>
    <title>GameBoy Color NES-themed modkit from FunnyPlaying</title>
    <link>https://jstolarek.github.io/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 23/01/2023
    </section>
    <section>
        <h1 id="gameboy-color-nes-themed-modkit-from-funnyplaying">GameBoy Color NES-themed modkit from FunnyPlaying</h1>
<center>
<strong>DISCLAIMER:</strong> This post is not intended to serve as installation tutorial.
</center>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/owned_gameboys.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/owned_gameboys_thumbnail.jpg" alt="owned_gb" /></a>
<figcaption>
GameBoys from my collection.
</figcaption>
</figure>
</center>
<p>Above photo has three GameBoys from my collection, left to right:</p>
<ol type="1">
<li><p>Unmodded GameBoy Color (GBC) in teal shell. It has original shell as well
as original screen. Note that the LCD screens back in the 90s and early
2000s didn’t have a backlight. This means that the only reasonable way of
playing is by having some light directed at the screen. Personally I found
playing outdoors on a sunny summer day to be the best experience.</p></li>
<li><p>Modded GameBoy Color in berry shell. This GameBoy had its screen replaced
with a modern, backlighted IPS panel. It also has a replaced power board
(more on the below) and the shell is also new.</p></li>
<li><p>Modded GameBoy Pocket (GBP) in clear shell. Again, original
black-and-white screen has been replaced with a backlighted IPS panel.
Shell and buttons are also new.</p></li>
</ol>
<p>Note the screen sizes. GBC, while offering colours, has a smaller screen area
compared to GBP and original DMG GameBoy. On the other hand, modern IPS screen
for GBP/DMG isn’t black-and-white - it just pretends to be one, since it only
receives monochromatic signal from the console. It was only a matter of time
before someone came up with the idea of putting the IPS screen designed for GBP
into GBC, thus giving us GameBoy Color with screen as large as in the GBP and
DMG models.</p>
<p>I didn’t have a GBC with larger screen in my collection yet, so I decided to
make one. After buying a new GBC to act as donor console I went to order a
screen. My go-to destination for GameBoy IPS panels is FunnyPlaying, a Chinese
company that over the years has shown commitment to making good products and
innovating their mods. Towards the end of last year they releases a <a href="https://funnyplaying.com/collections/product/products/gbc-nes-ves-retro-pixel-lcd-kit">NES-themed
GameBoy Color
modkit</a>
that not only contains an IPS panel but also comes with NES-themed shell and
all the other parts such as buttons and silicon pads. I searched for reviews
and opinions on this kit but couldn’t find any, since the kit was newly
released. I decided to give it a try and ordered it. While the package from
China arrived quickly, I took me quite a while to find the time to assemble it.
Without further ado, lets jump right in.</p>
<h2 id="kit-overview">Kit overview</h2>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/kit.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/kit_thumbnail.jpg" alt="fp_kit" /></a>
<figcaption>
Kit parts.
</figcaption>
</figure>
</center>
<p>The kit contains:</p>
<ul>
<li>A NES-themed shell</li>
<li>Laminated screen with a ribbon cable and wires, all stored in a stylish
plastic case modelled on GB cartridge. Note that there are two versions of
the kit: with standard glass lens and with 2.5D lens, which is rounded at
the edges. My kit is the latter one.</li>
<li>Buttons and silicon pads</li>
<li>Screws</li>
<li>Back sticker. This sticker is very high quality. It’s not printed on paper
but on a thick and durable metal-like material.</li>
</ul>
<p>The photo also includes a replacement speaker, which I bought as an extra - it
is not part of the kit! Here’s one more photo of the kit with the donor GBC and
unpacked screen:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/kit_2.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/kit_2_thumbnail.jpg" alt="fp_kit" /></a>
<figcaption>
Unpacked kit parts and donor GBC.
</figcaption>
</figure>
</center>
<p>One thing I quickly noticed is that the front and back of the shell don’t seem
to fit together well:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/shell_fit.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/shell_fit_thumbnail.jpg" alt="shell_fit" /></a>
<figcaption>
Shell fit seemed poor at first glance.
</figcaption>
</figure>
</center>
<p>This was a big surprise since all FunnyPlaying shells I had before had very good
fit and didn’t cause any issues. I was seriously worried at this point but I
need to get ahead of myself and say that after assembling the kit everything
fits nicely and my initial concerns were unwarranted.</p>
<h2 id="putting-the-kit-together">Putting the kit together</h2>
<p>My plan when making this mod was to also replace the original power board<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
The problem with original power boards is that they are not very efficient and
can struggle with higher power draw from IPS panel. This was an issue in my
berry GBC (middle one in the first photo), where the screen would randomly dim
to lowest brightness setting. That problem was solved by replacing original
power board with a modern one. I decided to do the same in this unit as well.</p>
<p>I began by disassembling the shell, disconnecting the screen ribbon and removing
the main board from the shell. Then I desoldered the power board and the
speaker:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/before_desoldering.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/before_desoldering_thumbnail.jpg" alt="shell_fit" /></a>
<figcaption>
Before desoldering.
</figcaption>
</figure>
</center>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/desoldering_done.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/desoldering_done_thumbnail.jpg" alt="_" /></a>
<figcaption>
After desoldering the speaker and power board.
</figcaption>
</figure>
</center>
<p>I don’t have a desoldering gun and overall don’t feel too confident with
desoldering. This was something that has caused me the most trouble with past
mods, sometimes even requiring me to destroy the part being desoldered. That
being said, things went much smother this time and I managed to desolder both
the speaker and the power board without damaging them in any way.</p>
<p>Next I soldered in the speaker and the new power board:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/soldered_power_board.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/soldered_power_board_thumbnail.jpg" alt="_" /></a>
<figcaption>
New power board and speaker soldered in place.
</figcaption>
</figure>
</center>
<p>FunnyPlaying instructs clients to test the IPS panel before soldering anything
to the ribbon like so:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/screen_test.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/screen_test_thumbnail.jpg" alt="_" /></a>
<figcaption>
Testing the screen when building the berry GBC.
</figcaption>
</figure>
</center>
<p>This was my next step. I connected the screen via the ribbon cable, attached
the back of the shell, which contains the battery compartment, powered on the
console and… nothing. The console seemed dead. No sound, no image on the
screen, and no light from the power LED. As the first debugging step I restored
the original speaker. After powering on the console sound was working, but the
screen still didn’t display anything. This has caused some serious head
scratching for me. At first I thought that perhaps the power board replacement
isn’t compatible with the IPS panel I was using, but then it dawned on me. The
screen mod used in this kit requires soldering a wire between the ribbon cable
and the power switch. After reading the description on the FunnyPlaying’s
website it turns out that the ribbon cable has a built-in power supply and draws
current directly from the power switch to bypass the original power board. So
it seems that the modern power board that I installed might be completely
redundant. It also looks that it’s not possible to carry out FunnyPlaying’s
instructions of testing the board without soldering any wires. I decided to
take a leap of faith and solder all the wires where they should be. But before
proceeding with the wires I trimmed the pins on the cartridge slot:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/cartridge_slot_pins.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/cartridge_slot_pins_thumbnail.jpg" alt="_" /></a>
<figcaption>
Trimmed cartridge slot pins.
</figcaption>
</figure>
</center>
<p>While the instructions on FP website don’t mention this as a required step, I
think this is a reasonable safety measure to prevent the pins from pushing on
the ribbon cable. Other screen mods I made required this and I didn’t want to
take any risks. Next I installed the screen in the shell. This kit comes with
a laminated screen, meaning that the glass is already fixed to the IPS panel.
There’s no need to worry about screen alignment or any dust getting under the
glass. Once screen was in place I connected the ribbon cable to the motherboard
and soldered the wires for Select and Start buttons:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/soldered_wires_1.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/soldered_wires_1_thumbnail.jpg" alt="_" /></a>
<figcaption>
Wires soldered and secured with Kapton tape.
</figcaption>
</figure>
</center>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/soldered_wires_2.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/soldered_wires_2_thumbnail.jpg" alt="_" /></a>
<figcaption>
Silicon pads in place. Note the security sticker is missing on the
back of the screen.
</figcaption>
</figure>
</center>
<p>As you can see I used lots of Kapton tape to:</p>
<ol type="1">
<li><p>Attach the ribbon cable to the back of the screen, so that it doesn’t move
around, and cover the exposed parts on top.</p></li>
<li><p>Attach Start and Select wires in place since they are significantly longer
than they need to be. In hindsight, I should have made those wires
shorter.</p></li>
<li><p>Secure cartridge slot pins.</p></li>
</ol>
<p>There’s one thing done incorrectly in these photos and I only realized that
after assembling the shell. Kit comes with a protective sticker that’s supposed
to be attached to the back of the screen to isolate the metal back from the
ribbon cable. There’s also a smaller sticker that, I’m guessing, is intended to
secure cartridge pins, but I already used Kapton tape for that. I didn’t want
to take any risk, so I disassembled the shell and applied the sticker, at least
to the degree it was possible.</p>
<p>Finally, I soldered the power cable and touch sensor, assembled the shell, and
applied the sticker to the back of the shell. Final result looks like this:</p>
<center>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/final_result_3.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/final_result_3_thumbnail.jpg" alt="_" /></a>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/final_result_2.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/final_result_2_thumbnail.jpg" alt="_" /></a>
<figcaption>
Front and back.
</figcaption>
</center>
<p>Here’s my new GameBoy next to the ones I already owned:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/final_result_1.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/final_result_1_thumbnail.jpg" alt="_" /></a>
<figcaption>
My current collection of GameBoy Colors.
</figcaption>
</figure>
</center>
<h2 id="summary">Summary</h2>
<p>This kit was a very nice build. I really liked the laminated screen. It makes
building a lot faster and I no longer need to worry about dust or getting my
fingerprints where they shouldn’t be. The screen is also small in size so it
doesn’t require cutting the the silicon pads, as was the case with earlier
screens from FunnyPlaying:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/cutting_silicon_pads.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/cutting_silicon_pads_thumbnail.jpg" alt="_" /></a>
<figcaption>
Cutting silicon pad to accommodate larger IPS panel.
</figcaption>
</figure>
</center>
<p>Obviously, the shell is also designed specifically for this mod and doesn’t
require trimming the plastic. This is nice progress compared to earlier mods.</p>
<p>Of course if I were to build that kit again it would take me much less time. I
wasted quite a lot of time on figuring out why the screen doesn’t power on. It
also seems that the replacement power board isn’t necessary so I could have
skipped that as well.</p>
<p>I’m not sure why the replacement speaker didn’t work. I probably should have
given it another try after figuring out the screen issue, but at this point the
build was taken much longer than anticipated and I wanted to finish it ASAP.</p>
<p>One thing I was concerned about when buying the kit was the logo under the
screen. That logo is backlighted by the IPS panel itself. I was concerned that
it will be distracting while playing, but it really isn’t. And in case it is,
it can be disabled in the settings.</p>
<p>Speaking of settings, the screen offers several brightness levels (adjusted
using the touch sensor on top edge of the shell), several vertical scanline
modes as well as RetroPixel mode. In this mode the screen displays a grid
that imitates pixel grid seen on older displays:</p>
<center>
<figure>
<a href="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/retro_pixels.jpg"><img src="/images/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying/retro_pixels_thumbnail.jpg" alt="_" /></a>
<figcaption>
RetroPixels mode in action.
</figcaption>
</figure>
</center>
<p>This is possible because the IPS panel has higher pixel density, with each
original pixel being represented by a 5 by 5 square of pixels. According to
information on FunnyPlaying’s website and other places on the web it should also
be possible to adjust vertical and horizontal position of the display area, but
for some reason this does not work in my unit. Perhaps these settings are
disabled when laminated screen is used?</p>
<p>Overall, I am very happy with the final result. The shell looks great and the
larger screen is really impressive. I am yet to test how long the batteries
last.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the power board is made by a company that has <a href="https://old.reddit.com/r/Gameboy/comments/ktbgnf/retrosix_is_stealing_other_peoples_work_and/">very bad
reputation in the retro
community</a>
and I have made a point not to buy from them again.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Mon, 23 Jan 2023 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2023-01-23-gameboy_color_nes-themed_modkit_from_funnyplaying.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>Playing Final Fantasy IX for the first time in 2022</title>
    <link>https://jstolarek.github.io/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 11/01/2023
    </section>
    <section>
        <h1 id="playing-final-fantasy-ix-for-the-first-time-in-2022">Playing Final Fantasy IX for the first time in 2022</h1>
<p>My gaming in 2022 wad dominated by two franchises: Final Fantasy and Metroid. I
re-played Super Metroid and Final Fantasy VI on original hardware, finished
Metroid Dread and Final Fantasy VII Remake, both latest entries in the series.
Finally, I managed to beat Samues: Metroid Returns, Metroid Prime 2 and Final
Fantasy IX. Today I want to talk about Final Fantasy IX, while it remains fresh
in my mind, leaving other mentioned games for possible retrospective posts
sometime in the future. I have had FFIX on my bucket list for a couple of years
now. I made one attempt at playing it around three years ago, but stalled about
four hours into the game. Four weeks ago I made a completely spontaneous
decision to play FFIX and this time the playthrough went smoothly right to the
end.</p>
<center>
<figure>
<a href="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/physical_copy.jpg"><img src="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/physical_copy_thumbnail.jpg" alt="final_fantasy_ix_physical_copy" /></a>
<figcaption>
A Japanese copy of Final Fantasy IX in my collection. Game comes on
four CDs.
</figcaption>
</figure>
</center>
<p>Final Fantasy IX is sometimes described as a “love letter to earlier entries in
the series” and a “return to the roots”. Indeed, after three earlier
instalments being placed in worlds that were more and more gravitating towards
sci-fi setting, this entry returns to fantasy roots seen in first five parts of
Final Fantasy. Of course when I say “fantasy” I mean “fantasy with airships”.
It wouldn’t be Final Fantasy otherwise!</p>
<center>
<figure>
<a href="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/title_screen.png"><img src="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/title_screen_thumbnail.jpg" alt="final_fantasy_ix_title_screen" /></a>
<figcaption>
Final Fantasy IX title screen.
</figcaption>
</figure>
</center>
<p>Final Fantasy IX has interesting story and great character writing. It was
Final Fantasy VII that became famous for their characters (few gamers haven’t
heard about Cloud or Aerith), but FFIX is equally good. Characters have their
own unique personalities and motivations, with my personal favourite being Vivi.
One notable exception to quality of character writing is Amarant. He has no
backstory, no purpose, no motivation behind his actions, and almost no
personality. Many of the game’s NPCs are written in more detail than him.
Amarant only serves as a pretext to have some lines about importance of working
with others. Overall he feels like an afterthought and the game would probably
be better without him. I also have a minor nitpick about Eiko. While she’s a
great character, according to the game story she’s only six years old. Except
that her words and actions certainly don’t fit a child. In particular her
initial fixation on having a romantic relationship with Zidane is weird, to say
the least.</p>
<center>
<figure>
<a href="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/gameplay.png"><img src="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/gameplay_thumbnail.jpg" alt="final_fantasy_ix_steiner" /></a>
<figcaption>
Just Steiner things.
</figcaption>
</figure>
</center>
<p>When it comes to character progression system, each character has an assigned
class. Those classes will be familiar to FF veterans (Black Mage, White Mage,
Thief, etc.), but in FFIX these classes come with a twist as each character now
has access to two types of class-specific commands. And so for example Zidane,
Freya, and Amarant have their typical class-based commands (Steal for thief
Zidane, Jump for dragon knight Freya and Throw for ninja Amarant) but each of
them also has access to unique spells only they can use in battle. Similarly
white mages Dagger and Eiko are now also Summoners, each of them having a
different set of summoning spells, while black mage Vivi can Focus to make his
spell in the next turn more powerful. Each character can also learn static
abilities that can be equipped from the menu between battles:</p>
<center>
<figure>
<a href="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/abilities.png"><img src="/images/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022/abilities_thumbnail.jpg" alt="final_fantasy_ix_abilities_menu" /></a>
<figcaption>
Early game abilities.
</figcaption>
</figure>
</center>
<p>All abilities - both class-specific spells as well as static abilities - are
learned from equipment items (weapons, armours, accessories, etc.), similarly to
how magic was learned from Espers in Final Fantasy VI. And similarly to magic
system from FFVI I often found myself changing the equipment after every single
battle because one of the characters learned a new ability and I wanted to equip
them with something different so that they begin learning a new ability. There
were moments when this became a bit tedious but overall it wasn’t a big deal.
Overall the ability system is fairly simple and doesn’t really offer any freedom
when it comes to character builds.</p>
<p>When it comes to battles, Final Fantasy IX uses a classic turn based ATB (Active
Time Battle) system, where characters wait until their action gauge fills up
(how fast this happens depends on character’s speed) and once it does they can
execute their action. The “Active” part of ATB means that enemies are allowed
to attack while the player is selecting actions from the menu, putting some
extra pressure on the player to act fast. This however always remained optional
and when playing Final Fantasies I have always disabled the “active” part of
ATB, allowing me to pick actions on my turn without rush.</p>
<p>Unfortunately, I don’t have anything good to say about implementation of ATB in
Final Fantasy IX. Earlier entries in the series got the ATB system right but
for some reason FFIX gets it wrong. In FFIX enemies seem to be largely
unaffected by ATB turn order. They perform actions at arbitrary moments, often
attacking several times in a row or in between series of attacks that one would
think should not be interruptible, e.g. character casting two spells as part of
Trance. (More on Trance below.) This completely destroys flow of battle and
derails decision making process in player’s head. In the time between the
decision to execute a particular action to the time of actually executing that
decision situation on the battlefield can change completely. This is further
exacerbated by various time-based effects, such as poison or regen, that work in
real-time rather than working in turn-time. This completely breaks balance of
poison an regen, among other things. For example, your character gets poisoned
and you use Antidote on them immediately. Oh, but some time earlier you also
decided to cast Bahamut, a powerful summon spell with lengthy animation, and it
gets cast now. Well, bad luck. Poisoned character will likely die because
their life drops every few seconds during summon animation. Conversely, Regen
status has become extremely overpowered. Especially with Auto-Regen ability
characters are able to recover most of their HP during spell animations, making
end-game significantly easier than it probably should be. And lastly,
pre-emptive strikes are also broken. During a pre-emptive strike your party
attacks enemy from behind - this happens randomly once in a while - and all of
your party members should be able to execute their attacks before enemies’ ATB
timer even starts ticking. Again, this is not the case in Final Fantasy IX.
During a pre-emptive strike it is not uncommon to have enemy attack right after
your first attack and before the rest of your team gets a chance to attack, thus
completely missing the point of a pre-emptive strike.</p>
<p>And then there’s questionable Trance mechanic to top it all. Trance is like
Limit Break in Final Fantasy VII or Overdrive in Final Fantasy X. As character
gets attacked a special gauge fills up and once the gauge is full character gets
access to a powerful special move. Trance is a bit different in that, rather
than being a single move, it lasts for several turns and for its duration allows
to execute special moves. For example Vivi can cast two spells on his turn
while Dagger’s summoning spells become more powerful. This is all fine. The
real problem with Trance is that it activates automatically when the Trance
gauge is full. The player has therefore absolutely no control over Trance
activation. As a result Trances get activated at completely random times, for
example during irrelevant world map battle, or right before a boss fight, or
right at the end of a fight giving character no chance to make any use of
Trance. As a result player has not strategic control over Trance and the whole
mechanic becomes a completely random bonus that only works occasionally. This
is in stark contrast with Limit Breaks in Final Fantasy VII, where charging them
up was one of techniques to help with boss fights.</p>
<p>Another aspect of the game that I also found slightly disappointing is the
music. As usually it was composed by Nobuo Uematsu but, compared to his other
works, music in FFIX feels very average. There are memorable themes once in a
while - Freya’s theme is a masterpiece! - but the music tracks player gets to
hear most often - battle theme, boss theme, world map theme - are meh.
Apparently some music tracks are adaptations of themes from Final Fantasies I,
II, and III, but I haven’t played those entries yet and only realized this from
reading the internet.</p>
<p>As with other Final Fantasy games, this one also has a lot of sidequests. I
have to admit I spectacularly missed some of them, including all Chocobo quests.
I also completely ignored the existence of Tetra Master mini card game, which I
found quite confusing and didn’t bother to search for rule explanation online.
Despite that the game took me 47 hours start to finish, which is more than it
took me to beat Final Fantasy VII when I replayed it four years ago. I played
most of Final Fantasy IX by emulating on Recalbox with RGB Dual. As outlined
<a href="https://jstolarek.github.io/posts/2022-12-26-recalbox-rgb-dual-some-impressions-after-six-months-of-usage.html">in my earlier
post</a>
I’ve run into performance problems, especially on the world map. I won’t hide
this hindered my experience and was one of the reasons I wasn’t keen on
exploring the world if I didn’t have to. For the last quarter of the game I
transferred the save file to my PS Vita, which required using online save
converter since the memory card format is different on the Vita, and played
while on a Christmas holiday trip. I gotta say this was a pretty fun thing to
do. I love how emulation and modding allow to achieve things not possible with
original hardware.</p>
<p>Well, that was quite a bit of complaining on my side. But do I regret playing?
Absolutely not! The story, characters, and the presented world make up for all
the shortcomings of battle system and unexceptional skill system. This is
definitely a good entry in the series and I am very happy to have finally played
it. I regret that I was unable to play FFIX 20 years ago when it came out. But
even playing it now was an unforgettable experience.</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Wed, 11 Jan 2023 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2023-01-11-playing_final_fantasy-ix-for-the-first-time-in-2022.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>Recalbox RGB Dual&#58; some impressions after six months of usage</title>
    <link>https://jstolarek.github.io/posts/2022-12-26-recalbox-rgb-dual-some-impressions-after-six-months-of-usage.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 26/12/2022
    </section>
    <section>
        <h1 id="recalbox-rgb-dual-some-impressions-after-six-months-of-usage">Recalbox RGB Dual: some impressions after six months of usage</h1>
<p>I am not a fan of Kickstarter and tend to stay away from it. That being said,
once in a while I make an exception. Typically my exceptions are books on retro
gaming, but towards the end of last year I decided to back campaign for
<a href="https://www.kickstarter.com/projects/recalbox/recalbox-rgb-dual">Recalbox RGB
Dual</a>, a
RaspberryPi HAT (Hardware Attached on Top) that provides SCART and VGA output
ports allowing to connect the Pi to a CRT TV or a VGA monitor:</p>
<center>
<a href="/images/posts/2022-12-26-rgb-dual/rgb_dual_1.jpg"><img src="/images/posts/2022-12-26-rgb-dual/rgb_dual_1_thumbnail.jpg" alt="rpi_heatsink" /></a>
<a href="/images/posts/2022-12-26-rgb-dual/rgb_dual_2.jpg"><img src="/images/posts/2022-12-26-rgb-dual/rgb_dual_2_thumbnail.jpg" alt="rpi_heatsink" /></a>
</center>
<p>RGB Dual allows setting up RaspberryPi as a dedicated retro emulation device and
running games on a proper CRT screen, ensuring that these old games look exactly
the way they were intended to. In the last couple of years I transitioned
towards playing on original hardware and I emulate less than I used to, but
nevertheless there’s still some retro hardware that I need to emulate:</p>
<ol type="1">
<li><p>Playstation 1, a.k.a. PSX. I haven’t pulled the trigger on this one yet.
Primary reasons are that mods are expensive and difficult (i.e. require
precision soldering I’m not confident with), while emulation is accurate
and, unlike Sega Saturn from the same generation, not computationally
expensive. Raspberry Pi has enough power to run PSX emulation at full
speed. Or at least it should. As you’ll see below this isn’t necessarily
the case.</p></li>
<li><p>NeoGeo MVS, CPS-1/2/3, and other arcade hardware. In principle, collecting
original arcade hardware isn’t easy due to its sizes and prices. Towards
the end of 2019 I was close to getting a consolized MVS, i.e. a NeoGeo MVS
arcade board adapted to be used at home connected to a for a reasonable
price. This is a relatively cheap and accessible way of owning original
NeoGeo arcade. Sadly, I didn’t decide on the purchase and during pandemic
the prices skyrocketed. Now consolized MVS costs 4-5 times more than it
used to in 2019.</p></li>
<li><p>Sega MegaDrive (and its CD variant). Not too many MegaDrive games I really
like so this one isn’t a priority for me.</p></li>
<li><p>NES. I’ve been omitting this one due to no RGB output on the console,
though this has recently changed thanks to Krikkz’s <a href="https://krikzz.com/our-products/cartridges/rgb-blaster.html">RGB
Blaster</a>.</p></li>
</ol>
<p>There are a bunch of other 8-bit and 16-bit systems that I’d like to explore one
day without having to buy the actual hardware, so RGB Dual definitely looked
like a good investment. I backed the campaign and patiently waited for the
delivery. After a tolerable delay of 2 months I got my board in June. The
first obstacle was getting RGB Dual to fit on top of Aluminium Armor heatsink.
This is a large heatsink that allows to efficiently cool the Pi silently with no
fans:</p>
<center>
<a href="/images/posts/2022-12-26-rgb-dual/rpi4_heatsink_1.jpg"><img src="/images/posts/2022-12-26-rgb-dual/rpi4_heatsink_1_thumbnail.jpg" alt="rpi_heatsink" /></a>
<a href="/images/posts/2022-12-26-rgb-dual/rpi4_heatsink_2.jpg"><img src="/images/posts/2022-12-26-rgb-dual/rpi4_heatsink_2_thumbnail.jpg" alt="rpi_heatsink" /></a>
</center>
<p>While I love these heatsinks and have them on all of my Pis I must admit their
height is a bit of a problem when connecting HATs. After some initial fitting I
trimmed the bottom pins on RGB Dual board with pliers and secured them with
Kapton tape to make sure they don’t touch the heatsink. I also gently trimmed
plastic SCART port mountings:</p>
<center>
<a href="/images/posts/2022-12-26-rgb-dual/trimmed_pins.jpg"><img src="/images/posts/2022-12-26-rgb-dual/trimmed_pins_thumbnail.jpg" alt="rpi_heatsink" /></a>
</center>
<p>With these adjustments the GPIO connector still doesn’t go all the way down but
it fits well enough for RGB Dual to operate safely:</p>
<center>
<a href="/images/posts/2022-12-26-rgb-dual/rgb_dual_mounted_1.jpg"><img src="/images/posts/2022-12-26-rgb-dual/rgb_dual_mounted_1_thumbnail.jpg" alt="Raspberry Pi with RGB Dual mounted on top" /></a>
<a href="/images/posts/2022-12-26-rgb-dual/rgb_dual_mounted_2.jpg"><img src="/images/posts/2022-12-26-rgb-dual/rgb_dual_mounted_2_thumbnail.jpg" alt="Raspberry Pi with RGB Dual mounted on top" /></a>
<a href="/images/posts/2022-12-26-rgb-dual/rgb_dual_mounted_3.jpg"><img src="/images/posts/2022-12-26-rgb-dual/rgb_dual_mounted_3_thumbnail.jpg" alt="Raspberry Pi with RGB Dual mounted on top" /></a>
</center>
<p>I could push it deeper but this comes at a cost of slightly bending the board
upwards, which I want to avoid not to weaken the solder joints on GPIO
connector.</p>
<p>Having fitted the board I began software setup. RGB Dual works only with
<a href="https://www.recalbox.com/">Recalbox</a>, a Linux distribution dedicated to
emulation that runs <a href="https://www.libretro.com/">RetroArch</a> under the hood. I
flashed a memory card, put it into the Pi, connected to my TV via SCART
port… and got a black screen. As a first debugging step I tried connecting
the Pi to HDMI monitor and, surely enough, everything Just Worked. I was
convinced that my board was dead on arrival. After spending 2 hours on
debugging it turned out that I need a special build of Recalbox distribution
that supports RGB Dual. Too bad this wasn’t mentioned in any obvious place -
this would have saved me some time. Note also that this is no longer the case
as of the moment of this writing. Mainline Recalbox distribution now supports
RGB Dual without any problems.</p>
<p>With everything ready I could now properly test how Recalbox works with the
board. Long story short, it does what it advertises but not without issues.
While I had no problems with arcade and NES emulation, I’ve run into problems
with PSX emulation. As mentioned earlier, Recalbox is based on RetroArch. By
default RetroArch offers multiple cores for emulating Playstation, such as
Beetle PSX (a fork of <a href="https://mednafen.github.io/">Mednafen</a>), <a href="https://github.com/notaz/pcsx_rearmed">PCSX
ReARMed</a>, or Swanstation (a fork of
<a href="https://www.duckstation.org/">Duckstation</a>). However, for the purpose of RGB
Dual developers of Recalbox limited available PSX cores to just a single one:
Swanstation. The rationale behind this is that having just a single core allows
to offer better support and this definitely sounds like reasonable approach on a
new product. Let me add here that Duckstation, while relatively new, offers
very high compatibility and in the recent years seems to have become a go-to
Playstation emulator. (I’m saying this based on reading discussions online, not
from my experience. I personally never used it, always relying on Beetle PSX.)
All that being said, PSX emulation quality offered by Swanstation in Recalbox is
below my expectations. First and foremost there is no support for 480i
resolution. As a result games that heavily rely on switching resolutions
between 240p and 480i, e.g. Chrono Cross, look bad when using the latter since
every other line is missing. Note that this is not a hardware limitation of RGB
Dual: it supports 480i resolution in the main Recalbox menu without problems.
Developers are promising 480i support in future versions of Recalbox but at the
moment this support is missing.</p>
<p>I have also run into serious performance issues with Swanstation. I am
currently playing Final Fantasy IX and while the game runs well in locations
with 2D backgrounds, it stutters horribly on the 3D World Map with framerates
going down from 60 frames per second to around 45. Framerate drops also
sometimes happen during battles, either when there are many (4-5) enemies or
when backgrounds are more complex than usual (e.g. contain animated waterfall).
Lastly, most FMVs stutter. The only way of improving the situation that I found
is disabling VSync. Doing this allows to boost FPS count on the world map to
around 55 but comes at a cost of occasional image flickering. This is very
disappointing. Raspberry Pi 4 certainly has enough power to run PSX emulation
at full speed. I’ve run Beetle PSX and PCSX ReARMed cores on
<a href="https://lakka.tv/">Lakka</a> and never experienced any performance problems. I am
guessing this higher emulation overhead comes from Swanstation’s alleged higher
accuracy, but again I have never experienced any accuracy issues under other
cores while Swanstation freezes every few hours of gameplay. Luckily, Recalbox
developers are promising more PSX cores compatible with RGB Dual in future
updates and I hope these new cores will solve the performance problems. I am
eagerly waiting for new releases of Recalbox. For now I just have to make do
with Swanstation. Too bad PSX emulation is my primary use case for RGB Dual and
as of this writing it leaves a lot to be desired.</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Mon, 26 Dec 2022 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2022-12-26-recalbox-rgb-dual-some-impressions-after-six-months-of-usage.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>Reactivation</title>
    <link>https://jstolarek.github.io/posts/2022-12-26-reactivation.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 26/12/2022
    </section>
    <section>
        <h1 id="reactivation">Reactivation</h1>
<p>I’ve been meaning to write this post for quite some time and now I finally have
some time to do it during Christmas break. My blog has been inactive for the
past six years, coming to a complete stop when I moved to Edinburgh in 2016.
Reasons behind this are quite simple: I had more research work on my head at the
University of Edinburgh. This resulted in me having less time and interest in
blogging.</p>
<p>A year ago I made a decision to return to Poland. Last year at this time I was
finishing my preparations to leave the UK. This was a hectic time and I’m glad
I won’t have to move between countries in any foreseeable future. Decision to
return to my home country was combined with changing jobs. I left the academia
and moved to the industry to work as a Haskell developer. As part of that
process I had to salvage my blog posts which were still stored in a dilapidated
Wordpress instance on my University’s server. It was tedious but after several
evenings I had all the posts as Markdown files, my homepage now rewritten to use
Hakyll and moved to GitHub pages.</p>
<p>In the past weeks and months I’ve been thinking about returning to blogging.
It’s time to reactivate this blog, if only for a bit. I just need a place where
I can share/dump/record my thoughts on various things I’m into these days. This
means that posts will likely not focus on functional programming. Expect
writeups on retro hardware and all sort of games. The decision to reactivate my
blog was largely inspired by <a href="https://fabiensanglard.net/">Fabien Sanglard’s fantastic
blog</a>. I have no ambition to provide as good
technical insights as he does - though who knows, maybe one day? - but seeing
how he combines technical and casual posts in a single blog gave me the courage
to resume blogging.</p>
<p>I also decided I’m keeping the name - Yet Another Lambda Blog - though the
amount of lambdas might be much lower than it used to be in the past.</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Mon, 26 Dec 2022 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2022-12-26-reactivation.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>DeepSpec Summer School 2017 - a summary</title>
    <link>https://jstolarek.github.io/posts/2017-07-30-deepspec-summer-school-2017-a-summary.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 30/07/2017
    </section>
    <section>
        <h1 id="deepspec-summer-school-2017---a-summary">DeepSpec Summer School 2017 - a summary</h1>
<p>I have spent the last two and a half week in Philadelphia attending the first
<a href="https://deepspec.org/event/dsss17/">DeepSpec Summer School</a>, In this post I
want to summarize the event and give an overview of all the courses.</p>
<p><a href="https://deepspec.org">The DeepSpec Project</a> is a research project lead by
several US East Coast universities (University of Pennsylvania, MIT, Yale
University and Princeton University) and aims to <em>“push forward the state of the
art in applying computer proof assistants to verify realistic software and
hardware stacks at scale”</em>. It consists of <a href="https://deepspec.org/page/Project/">several smaller
projects</a>, including a formal verification
of a hypervisor (<a href="http://flint.cs.yale.edu/certikos/">CertiKOS</a>), LLVM
(<a href="http://www.cis.upenn.edu/~stevez/vellvm/">Vellvm</a>), Coq compiler
(<a href="https://www.cs.princeton.edu/~appel/certicoq/">CertiCoq</a>) and GHC’s Core
language (<a href="https://deepspec.org/entry/Project/Haskell+CoreSpec">CoreSpec</a>).</p>
<p>The goal of DeepSpec Summer School was to introduce people to real-life formal
verification using Coq proof assistant. School was divided into three parts. All
the lectures <a href="https://www.youtube.com/channel/UC5yB0ZRgc4A99ttkwer-dDw">can be found on a YouTube
channel</a>. Coq code for
the courses <a href="https://github.com/DeepSpec/dsss17">is available on GitHub</a>. Summer
school’s web page also provides <a href="https://deepspec.org/event/dsss17/installation.html">installation
instructions</a> as well as
<a href="https://deepspec.org/event/dsss17/schedule.html">other supplementary material</a>
(click on a given lecture or select from “Lectures” tab).</p>
<h1 id="week-0-coq-intensive">Week 0: Coq Intensive</h1>
<p>First three days of the summer school were a very intensive introductory course
on Coq lead by Benjamin Pierce. This essentially covered the first volume of
Software Foundations. (Aside: For those of you who don’t know yet, <a href="https://deepspec.org/page/SF/">original
Software Foundations online book has been split into two
volumes</a>: Logical Foundations and Programming
Language Foundations. Also, a third volume has been added to the series:
Verified Functional Algorithms by Andrew Appel. All three volumes can be found
<a href="https://softwarefoundations.cis.upenn.edu/draft/">here</a>, although expect that
this link will likely become broken soon when this draft versions become an
official release. There are also plans for two more volumes, one on Separation
Logic and another one on Systems Verification.)</p>
<h1 id="week-1-programming-language-verification">Week 1: Programming Language Verification</h1>
<p>First full week of the school consisted of four courses centred around
programming language verification:</p>
<ul>
<li><p><em>Property-based random testing with QuickChick</em> by Benjamin Pierce. I assume
many of you heard about Haskell library called QuickCh<strong>e</strong>ck. It offers
property-based testing: programmer writes properties that the should hold
for a given piece of code and QuickCheck tests whether they hold for
randomly generated test data. QuickCh<strong>i</strong>ck is implementation of the same
idea in Coq. Now, you might wonder what is the point of doing such a thing
in Coq. After all, Coq is about formally proving that a given property is
always true, not randomly testing whether it holds. I was sceptical about
this as well, but it actually turns to be quite a good idea. The point is,
specifications are difficult to write and often even more difficult to
prove. They are especially difficult to prove when they are false ;-) And
this is exactly when QuickChick can be beneficial: by trying to find a
counter-example for which a stated property does not hold. This can indeed
save programmer from spending hours on trying to prove something that is
false. If QuickChick doesn’t find a counter-example we can start writing a
formal proof. This course also gives a nice overview of type classes in Coq.</p></li>
<li><p><em>The structure of verified compiler</em> by Xavier Leroy. This series of
lectures was based on <a href="http://compcert.inria.fr/">CompCert</a>, which is a
formally verified C compiler. The ideas behind formal verification of a
compiler were presented on a compiler of Imp (a toy imperative language used
in Software Foundations) to a simple virtual machine. Fourth, final lecture
covered the CompCert project itself. To me this was the most interesting
course of the summer school.</p></li>
<li><p><em>Language specification and variable binding</em> by Stephanie Weirich. Software
Foundations is a great book, but it completely omits one topic that is very
important in formalizing programming languages: dealing with variable
bindings. In this courses Stephanie presented “locally nameless”
representation of variable bindings. This is something I had planned to
learn for a very long time but couldn’t find the time.</p></li>
<li><p><em>Vellvm: Verifying the LLVM</em> by Steve Zdancewic. For a change, in this
course Imp was compiled to a simplified variant of LLVM, the compilation
process being verified of course. Also, a nice introduction to LLVM.</p></li>
</ul>
<h1 id="week-2-systems-verification">Week 2: Systems Verification</h1>
<p>Courses during the second week put more focus on verifying computer
systems. Again, there were four courses:</p>
<ul>
<li><p><em>Certifying software with crashes</em> by Frans Kaashoek and Nickolai Zeldovich.
The topic of this course was certification of a hard-drive operating
routines, including bad-sector remapping and a simple virtual RAID 1
implementation. Although still using toy examples, specifications presented
during this course were much more abstract giving a good idea how to scale
to a real-world system verification. I found this course very difficult to
follow, although the lectures were really superb. Note: materials for this
one course are available <a href="https://github.com/mit-pdos/deepspec-pocs">in a separate GitHub
repo</a>.</p></li>
<li><p><em>CertiKOS: Certified kit operating systems</em> by Zhong Shao. Ok, I admit I was
completely unable to follow this series of lectures. Way to difficult. In
fact, I skipped two out of four lectures because I figured out it will make
more sense to work on homework assignments for other lectures.</p></li>
<li><p><em>Program-specific proof automation</em> by Adam Chlipala. Unsurprisingly to
those who know Adam’s <em>“Certified Programming with Dependent Types”</em> book,
his course focused on proof automation using Ltac. One lecture was
specifically dedicated to proofs by reflection.</p></li>
<li><p><em>Verified Functional Algorithms</em> by Andrew Appel. This course covered a
majority of third volume of new Software Foundations.</p></li>
</ul>
<h1 id="summary">Summary</h1>
<p>First and foremost let me say this: DeepSpec Summer School was the best research
meeting I have ever attended. The courses were really good and inspiring, but
the most important thing that made this summer school so great were fantastic
people who attended it. Spending evening hours together working on homework
assignments was especially enjoyable.</p>
<p>There might be a 2018 edition of the summer school so be on the lookout - this
is a really great event for anyone interested in Coq and formal verification.</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Sun, 30 Jul 2017 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2017-07-30-deepspec-summer-school-2017-a-summary.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>Moving to University of Edinburgh</title>
    <link>https://jstolarek.github.io/posts/2016-09-14-moving-to-university-of-edinburgh.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 14/09/2016
    </section>
    <section>
        <h1 id="moving-to-university-of-edinburgh">Moving to University of Edinburgh</h1>
<p>I wanted to let you all know that after working for 8 years as a Lecturer at the
Institute of Information Technology (Lodz University of Technology, Poland), I
have received a sabbatical leave to focus solely on research. Yesterday I began
my work as a Research Associate at the Laboratory for Foundations of Computer
Science, University of Edinburgh. This is a two-year post-doc position. I will
be part of the team working on the
<a href="http://homepages.inf.ed.ac.uk/jcheney/group/skye.html">Skye</a> project under
supervision of <a href="http://homepages.inf.ed.ac.uk/jcheney/">James Cheney</a>. This
means that from now on I will mostly focus on developing the <a href="http://links-lang.org/">Links programming
language</a>.</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Wed, 14 Sep 2016 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2016-09-14-moving-to-university-of-edinburgh.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>First impression of "Real World OCaml"</title>
    <link>https://jstolarek.github.io/posts/2016-08-06-first-impression-of-real-world-ocaml.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 06/08/2016
    </section>
    <section>
        <h1 id="first-impression-of-real-world-ocaml">First impression of “Real World OCaml”</h1>
<p>Tomorrow I will be flying to Cambridge to attend <a href="http://www.cl.cam.ac.uk/events/metaprog2016/">International Summer School on
Metaprogramming</a>. One of the
prerequisites required from the participants is basic knowledge of OCaml,
roughly the first nine chapters of “Real World OCaml” (RWO for short). I
finished reading them several days ago and thought I will share my impressions
about the book.</p>
<p><a href="/images/posts/rwo1.png"><img src="/images/posts/rwo1-228x300.png" alt="rwo" /></a></p>
<p>RWO was written by Yaron Minsky, <a href="http://anil.recoil.org/">Anil Madhavapeddy</a>
and Jason Hickey. It is one of a handful of books on OCaml. Other titles out
there are <a href="http://ocaml-book.com/">“OCaml from the Very Beginning”</a> and <a href="http://ocaml-book.com/more-ocaml-algorithms-methods-diversions/">“More
OCaml: Algorithms, Methods and
Diversions”</a> by
John Whitington and “Practical OCaml” by Joshua Smith. I decided to go with RWO
because when I asked “<em>what is the best book on OCaml</em>” on <code>#ocaml</code> IRC channel
RWO was an unanimous response from several users. The title itself is obviously
piggybacking on an earlier “Real World Haskell” released in the same series by
O’Reilly, which was in general a good book (<a href="/posts/2013-01-06-real-world-haskell-impressions-after-initial-chapters.html">though it had its
flaws</a>).</p>
<p>The first nine chapters comprise about 40% of the book (190 pages out of 470
total) and cover the basics of OCaml: various data types (lists, records,
variants), error handling, imperative programming (eg. mutable variables and
data structures, I/O) and basics of the module system. Chapters 10 through 12
present advanced features of the module system and introduce object-oriented
aspects of OCaml. Language ecosystem (ie. tools and libraries) is discussed in
chapters 13 through 18. The remaining chapters 19 through 23 go into details of
OCaml compiler like garbage collector or Foreign Function Interface.</p>
<p>When I think back about reading “Real World Haskell” I recall that quite a lot
of space was dedicated to explaining in detail various basic functional
programming concepts. “Real World OCaml” is much more dense. It approaches
teaching OCaml just as if it was another programming language, without making
big deal of functional programming model. I am much more experienced now than
when reading RWH four years ago and this is exactly what I wanted. I wonder
however how will this approach work for people new to functional programming. It
reminds my of my early days as a functional programmer. I began learning Scala
having previously learned Scheme and Erlang (both unusual for functional
languages in lacking a type system). Both Scala and OCaml are not pure
functional languages: they allow free mixing of functional and imperative
(side-effecting) code. They also support object-oriented programming. My plan in
learning Scala was to learn functional programming and I quickly realized that I
was failing. Scala simply offered too many back-doors that allowed escaping into
the imperative world. So instead of forcing me to learn a new way of thinking it
allowed me to do things the old way. OCaml seems to be exactly the same in this
regard and RWO offers beginners little guidance to thinking
functionally. Instead, it gives them a full arsenal of imperative features early
on in the book. I am not entirely convinced that this approach will work well
for people new to FP.</p>
<p>“Real World OCaml” was published less than three years ago so it is a fairly
recent book. Quite surprisingly then several sections have already gone out of
date. The code does not work with the latest version of OCaml compiler and
requires non-obvious changes to work. (You can of course solve the problem by
working with the old version of OCaml compiler.) I was told on IRC that the
authors are already working on the second edition of the book to bring it to
date with today’s OCaml implementation.</p>
<p>Given all the above my verdict on “Real World OCaml” is that it is a really good
book about OCaml itself (despite being slightly outdated) but not necessarily
the best book on basics of functional programming.</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Sat, 06 Aug 2016 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2016-08-06-first-impression-of-real-world-ocaml.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>Coq'Art, CPDT and SF&#58; a review of books on Coq proof assistant</title>
    <link>https://jstolarek.github.io/posts/2016-06-08-coqart-cpdt-and-sf-a-review-of-books-on-coq-proof-assistant.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 08/06/2016
    </section>
    <section>
        <h1 id="coqart-cpdt-and-sf-a-review-of-books-on-coq-proof-assistant">Coq’Art, CPDT and SF: a review of books on Coq proof assistant</h1>
<p>I have been pretty quiet on the blog in the past couple of months. One of the
reasons for this is that I have spent most of my time learning Coq. I had my
first contact with Coq well over a year ago <a href="/posts/2015-03-24-first-impressions-of-coq-and-certified-programming-with-dependent-types.html">when I started reading
CPDT</a>.
Back then I only wanted to learn the basics of Coq to see how it works and what
it has to offer compared to other languages with dependent types. This time I
wanted to apply Coq to some ideas I had at work, so I was determined to be much
more thorough in my learning. Coq is far from being a mainstream language but
nevertheless it has some really good learning resources. Today I would like to
present a brief overview of what I believe are the three most important books on
Coq: <em>“Interactive Theorem Proving and Program Development. Coq’Art: The
Calculus of Inductive Constructions”</em> (which I will briefly refer to as Coq’Art)
by Yves Bertot and Pierre Castéran, <em>“Certified Programming with Dependent
Types”</em> (CPDT) by Adam Chlipala and <em>“Software Foundations”</em> (SF for short) by
Benjamin Pierce and over a dozen over contributors. All three books
significantly differ in their scope and focus. CPDT and Coq’Art are standard,
printed books. CPDT is also <a href="http://adam.chlipala.net/cpdt/">available online for
free</a>. Software Foundations is only available
<a href="http://www.cis.upenn.edu/~bcpierce/sf/current/toc.html">as an online
book</a>. Interestingly,
<a href="http://www.cis.upenn.edu/~bcpierce/sf/sf-4.0/">there is also a version of SF that seems to be in the process of being
revised</a>.</p>
<p>I believe Coq’Art was the first book published on Coq. There are two editions -
2004 hardcover version and a 2010 paperback version - but as far as I know there
are no differences between them. Too bad the 2010 edition was not updated for
the newest versions of Coq - some of the code examples don’t work in the newest
compiler. Coq’Art takes a theoretical approach, ie. it teaches Coq largely by
explaining how the rules of Calculus of Constructions work. There are also
practical elements like case studies and exercises but they do not dominate the
book. Personally I found Coq’Art to be a very hard read. Not because it dives
too much in theory - it doesn’t - but because the presentation seems to be
chaotic. For example, description of a single tactic can be spread throughout
deveral places in the book. In principle, I don’t object to extending earlier
presentation with new details once the reader gets a hold of some new concepts,
but I feel that Coq’Art definitely goes too far. Coq’Art also presents material
in a very unusual order. Almost every introduction to Coq or any other
functional language begins with defining data types. Coq’Art introduces them in
chapter 6. On the other hand sorts and universes - something I would consider an
advanced concept for anyone who is not familiar with type-level programming -
are presented in the second chapter. (Note that first chapter is a very brief
overview of the language.) By contrast, CPDT goes into detailed discussion of
universes in chapter 12 and SF does not seem to cover them at all. Overall,
Coq’Art is of limited usefulness to me. To tell the truth this is not because of
its focus on theory rather than practice, but because of language style, which I
find rather inaccessible. Many times I had problems understanding passages I was
reading, forcing me to re-read them again and again, trying to figure out what
is the message that the authors are trying to convey. I did not have such
problems with CPDT, SF, nor any other book I have read in the past few years. At
the moment I have given up on the idea of reading the book from cover to
cover. Nevertheless I find Coq’Art a good supplementary reading for SF. Most
importantly because of the sections that explain in detail the inner workings of
various tactics.</p>
<p>As mentioned at the beginning, I already wrote a <a href="/posts/2015-03-24-first-impressions-of-coq-and-certified-programming-with-dependent-types.html">first impressions post about
CPDT</a>.
Back then I said the book “is a great demonstration of what can be done in Coq
but not a good explanation of how it can be done”. Having read all of it I
sustain my claim. CPDT does not provide a thorough and systematic coverage of
basics, but instead focuses on advanced topics. As such, it is not the best
place to start for beginners but it is a priceless resource for Coq
practitioners. The main focus of the book is proof automation with Ltac, Coq’s
language for writing automated proof procedures. Reader is exposed to Ltac early
on in the book, but detailed treatment of Ltac is delayed until chapter
14. Quite surprisingly, given that it is hard to understand earlier chapters
without knowing Ltac. Luckily, the chapters are fairly independent of each other
and can be read in any order the reader wishes. Definitely it is worth to dive
into chapter 14 and fragments of apter 13 as early as possible - it makes
understanding the book a whole lot easier. So far I have already read chapter 14
three times. As I learn Coq more and more I discover new bits of knowledge with
each read. In fact, I expect to be going back regularly to CPDT.</p>
<p>Coq’Art and CPDT approach teaching Coq in totally different ways. It might then
be surprising that Software Foundations uses yet another approach. Unlike
Coq’Art it is focused on practice and unlike CPDT it places a very strong
emphasis on learning the basics. I feel that SF makes Coq learning curve as flat
as possible. The main focus of SF is applying Coq to formalizing programming
languages semantics, especially their type systems. This should not come as a
big surprise given that Benjamin Pierce, the author of SF, authored also “<em>Types
and Programming Languages”</em> (TAPL), the best book on the topic of type systems
and programming language semantics I have seen. It should not also be surprising
that a huge chunk of material overlaps between TAPL and SF. I find this to be
amongst the best things about SF. All the proofs that I read in TAPL make a lot
more sense to me when I can convert them to a piece of code. This gives me a
much deeper insight into the meaning of lemmas and theorems. Also, when I get
stuck on an exercise I can take a look at TAPL to see what is the general idea
behind the proof I am implementing.</p>
<p>SF is packed with material and thus it is a very long read. Three months after
beginning the book and spending with it about two days a week I am halfway
through. The main strength of SF is a plethora of exercises. (Coq’Art has some
exercises, but not too many. CPDT has none). They can take a lot of time - and I
<em>really</em> mean a lot - but I think this is the only way to learn a programming
language. Besides, the exercises are very rewarding. One downside of the
exercises is that the book provides no solutions, which is bad for
self-studying. Moreover, the authors ask people not to publish the solutions on
the internet, since “having solutions easily available makes [SF] much less
useful for courses, which typically have graded homework assignments”. That
being said, there are plenty of github repositories that contain the solved
exercises (I also pledge guilty!). Although it goes against the authors’ will I
consider it a really good thing for self-study: many times I have been stuck on
exercises and was able to make progress only by peeking at someone else’s
solution. This doesn’t mean I copied the solutions. I just used them to overcome
difficulties and in some cases ended up with proofs more elegant than the ones I
have found. As a side note I’ll add that I do not share the belief that
publishing solutions on the web makes SF less useful for courses. Students who
want to cheat will get the solutions from other students anyway. At least that
has been my experience as an academic teacher.</p>
<p>To sum up, each of the books presents a different approach. Coq’Art focuses on
learning Coq by understanding its theoretical foundations. SF focuses on
learning Coq through practice. CPDT focuses on advanced techniques for proof
automation. Personally, I feel I’ve learned the most from SF, with CPDT closely
on the second place. YMMV</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Wed, 08 Jun 2016 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2016-06-08-coqart-cpdt-and-sf-a-review-of-books-on-coq-proof-assistant.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>Installing OCaml under openSUSE 11.4, or&#58; "the compilation of conf-ncurses failed"</title>
    <link>https://jstolarek.github.io/posts/2016-05-31-installing-ocaml-under-opensuse-11-4-or-the-compilation-of-conf-ncurses-failed.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 31/05/2016
    </section>
    <section>
        <h1 id="installing-ocaml-under-opensuse-11.4-or-the-compilation-of-conf-ncurses-failed">Installing OCaml under openSUSE 11.4, or: “the compilation of conf-ncurses failed”</h1>
<p>Recently I decided to learn the basics of OCaml and I spent yesterday installing
the compiler and some basic tools. On my machine at work I have a Debian 7
installation, while on my home laptop I have openSUSE 11.4. Both systems are
quite dated and ship with OCaml 3.x compiler, which is five years
old. Obviously, I wanted to have the latest version of the language. I could
have compiled OCaml from sources - and in fact I have done that in the past to
compile the latest version of Coq - but luckily there is a tool called OPAM
(OCaml Package manager). OPAM can be used to easily download and install desired
version of OCaml compiler. As the name implies, OPAM can be also used for
managing packages installed for a particular compiler version.</p>
<p>The installation process went very smooth on my Debian machine, but on openSUSE
I have run into problems. After getting the latest compiler I wanted to install
<code>ocamlfind</code>, a tool required by a project I wanted to play with. To my
disappointment installation ended with an error:</p>
<pre><code>[ERROR] The compilation of conf-ncurses failed at &quot;pkg-config ncurses&quot;.

This package relies on external (system) dependencies that may be missing.
`opam depext conf-ncurses.1&#39; may help you find the correct installation for
your system.</code></pre>
<p>I verified that I indeed have installed development files for the <code>ncurses</code>
library as well as the <code>pkg-config</code> tool. Running the suggested <code>opam</code> command
also didn’t find any missing dependencies, and the log files from the
installation turned out to be completely empty, so I was left clueless. Googling
revealed that <a href="https://github.com/ocaml/opam-repository/issues/5880">I am not the first to encounter this
problem</a>, but offered no
solution. I did some more reading on <code>pkg-config</code> and learned that: a) it is a
tool that provides meta-information about installed libraries, and b) in order
to recognize that a library is installed it requires extra configuration files
(aka <code>*.pc</code> files) provided by the library. Running <code>pkg-config --list-all</code>
revealed that <code>ncurses</code> is not recognized as installed on my system, which
suggested that the relevant <code>*.pc</code> files are missing. Some more googling
revealed that <code>ncurses</code> library can be configured and then compiled with
<code>--enable-pc-files</code> switch, which should build the files needed by
<code>pkg-config</code>. I got the sources for the <code>ncurses</code> version installed on my system
(5.7) only to learn that this build option is unsupported. This explains why the
files are missing on my system. I got the sources for the latest version of
<code>ncurses</code> (6.0), configured them with <code>--enable-pc-files</code> and compiled, only to
learn that the <code>*.pc</code> files were not built. After several minutes of debugging I
realized that for some unexplained reasons the <code>configure</code>-generated script
which should build the <code>*.pc</code> files (located at <code>misc/gen-pkgconfig</code>) did not
receive <code>+x</code> (executable) permission. After adding this permission manually I
ran the script and got five <code>*.pc</code> files for the <code>ncurses</code> 6.0 library. Then I
had to edit the files to match the version of <code>ncurses</code> of my system - relevant
information can be obtained by running <code>ncurses5-config --version</code>. The only
remaining thing was to place the five <code>*.pc</code> files in a place where <code>pkg-config</code>
can find them. On openSUSE this was <code>/usr/local/pkgconfig</code>, but this can differ
between various Linux flavours.</p>
<p>After all these magical incantations the installation of <code>ocamlfind</code> went
through fine and I can enjoy a working OCaml installation on both of my
machines. Now I’m waiting for the “Real-world OCaml” book ordered from Amazon
(orders shipped from UK Amazon to Poland tend to take around two weeks to
arrive).</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Tue, 31 May 2016 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2016-05-31-installing-ocaml-under-opensuse-11-4-or-the-compilation-of-conf-ncurses-failed.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>
<item>
    <title>Typed holes support in Template Haskell</title>
    <link>https://jstolarek.github.io/posts/2015-10-28-typed-holes-support-in-template-haskell.html</link>
    <description><![CDATA[<article>
    <section class="header">
        Posted on 28/10/2015
    </section>
    <section>
        <h1 id="typed-holes-support-in-template-haskell">Typed holes support in Template Haskell</h1>
<p>Back in April I found myself in a need for typed holes in Template Haskell. To
my disappointment it turned out that typed holes are not implemented in TH.
Sadly, this happens too often: a feature is added to GHC but no Template Haskell
support is implemented for it. This was the time when I was working on injective
type families and I already had some experience in extending TH
implementation. I figured that adding support for typed holes should be a
trivial task, no more than 30 minutes of coding. I created a <a href="https://ghc.haskell.org/trac/ghc/ticket/10267">feature request on
Trac</a> and started coding. I
quickly realized that it won’t be that simple. Not that the amount of required
work was that extensive. I simply tripped over the way GHC handles names
internally. As a result the work got stalled for several months and I only
finished it two weeks ago thanks to help from Richard Eisenberg.</p>
<p>My patch allows you to do several interesting things. Firstly, it allows to
quote typed holes, ie. expressions with name starting with an underscore:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>[d| i :: a -&gt; a</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    i x = _ |]</span></code></pre></div>
<p>This declaration quote will represent <code>_</code> using an <code>UnboundVarE</code>
constructor. Secondly, you can now splice unbound variables:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ot">i ::</span> a <span class="ot">-&gt;</span> a</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>i x <span class="ot">=</span> <span class="op">$</span>( <span class="fu">return</span> <span class="op">$</span> <span class="dt">VarE</span> (mkName <span class="st">&quot;_&quot;</span>) )</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="ot">j ::</span> a <span class="ot">-&gt;</span> a</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>j x <span class="ot">=</span> <span class="op">$</span>( <span class="fu">return</span> <span class="op">$</span> <span class="dt">UnboundVarE</span> (mkName <span class="st">&quot;_&quot;</span>) )</span></code></pre></div>
<p>Notice that in a splice you can use either <code>VarE</code> or <code>UnboundVarE</code> to represent
an unbound variable - they are treated the same.</p>
<p>A very important side-effect of my implementation is that you can actually quote
unbound variables. This means that you can now use nested pattern splices, as
demonstrated by one of the tests in GHC testsuite:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>baz <span class="ot">=</span> [<span class="op">|</span> \ <span class="op">$</span>( <span class="fu">return</span> <span class="op">$</span> <span class="dt">VarP</span> <span class="op">$</span> mkName <span class="st">&quot;x&quot;</span> ) <span class="ot">-&gt;</span> x <span class="op">|</span>]</span></code></pre></div>
<p>Previously this code was rejected. The reason is that:</p>
<ol type="1">
<li><p>nested pattern splice is not compiled immediately, because it is possible
that it refers to local variables defined outside of the bracket;</p></li>
<li><p>the bracket is renamed immediately at the declaration site and all the
variables were required to be in scope at that time.</p></li>
</ol>
<p>The combination of the above means that the pattern splice does not bring
anything into scope (because it is not compiled until the outer bracket is
spliced in), which lead to <code>x</code> being out of scope. But now it is perfectly fine
to have unbound variables in a bracket. So the above definition of <code>baz</code> is now
accepted. When it is first renamed <code>x</code> is treated as an unbound variable, which
is now fine, and when the bracket is spliced in, the inner splice is compiled
and it correctly brings binding for <code>x</code> into scope. Getting nested pattern
splices to work was not my intention when I started implementing this patch but
it turned out we essentially got this feature for free.</p>
<p>One stumbling block during my work was typed Template Haskell. With normal,
untyped TH I can place a splice at top-level in a file:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">$$</span>(<span class="fu">return</span> [</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>   <span class="dt">SigD</span> (mkName <span class="st">&quot;m&quot;</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        (<span class="dt">ForallT</span> [<span class="dt">PlainTV</span> (mkName <span class="st">&quot;a&quot;</span>)]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                 []</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                 (<span class="dt">AppT</span> (<span class="dt">AppT</span> <span class="dt">ArrowT</span> (<span class="dt">VarT</span> (mkName <span class="st">&quot;a&quot;</span>))) (<span class="dt">VarT</span> (mkName <span class="st">&quot;a&quot;</span>))))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a> , <span class="dt">FunD</span> (mkName <span class="st">&quot;m&quot;</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        [<span class="dt">Clause</span> [<span class="dt">VarP</span> (mkName <span class="st">&quot;x&quot;</span>)] (<span class="dt">NormalB</span> (<span class="dt">VarE</span> (mkName <span class="st">&quot;x&quot;</span>))) [] ]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>   ])</span></code></pre></div>
<p>and this will build a definition that will be spliced into the source code. But
converting this into a typed splice, by saying <code>$$(return ....</code>, resulted in
compiler panic. I reported this as
<a href="https://ghc.haskell.org/trac/ghc/ticket/10945">#10945</a>. The reason turned out
to be quite tricky. When Template Haskell is enabled, top-level expressions are
allowed. Each such expression is treated as an implicit splice. The problem with
typed TH splice is that it doesn’t really make sense at the top-level and it
should be treated as an implicit splice. Yet it was treated as an explicit
splice, which resulted in a panic later in the compiler pipeline.</p>
<p>Another issue that came up with typed TH was that typed holes cannot be quoted,
again leading to panic. I reported this as
<a href="https://ghc.haskell.org/trac/ghc/ticket/10946">#10946</a>. This issue has not yet
been solved.</p>
<p>The above work is now <a href="https://git.haskell.org/ghc.git/commitdiff/75492e7467ff962f2f2e29e5c8b2c588c94ae8a7">merged with
HEAD</a>
and will be available in GHC 8.0.</p>

        <p><a href="../blog.html">Back</a></p>
    </section>
</article>
]]></description>
    <pubDate>Wed, 28 Oct 2015 00:00:00 UT</pubDate>
    <guid>https://jstolarek.github.io/posts/2015-10-28-typed-holes-support-in-template-haskell.html</guid>
    <dc:creator>Jan Stolarek</dc:creator>
</item>

    </channel>
</rss>
